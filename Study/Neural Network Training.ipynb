{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd54f23-1d1d-4656-8756-b409e89609ec",
   "metadata": {},
   "source": [
    "# Neural Network Training\n",
    "\n",
    "In this chapter, \"training\" refers to automatixcally acquiring the optimal values of the weight parameters from the training data.\n",
    "The goal of training is to find the weight parameters that minimize the result of the loss function, which is a metric(지표) that allows neural networks to train.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "In the Neural Network Training, the loss function serves as a metric used to search for optimal parameter values.\n",
    "The loss function quantifies the difference between the model(=Neural Network)'s predictions and actual truth, thus playing a crucial role in evaluating the model's performance and suggesting improvement directions.\n",
    "By updating the model's parameters in the direction of minimizing the loss function, the predictive performance of the model is enhanced.\n",
    "As a loss function, arbitrary functions(임의의 함수) can be used, but generally used SSE(sum of squares of error)(오차제곱합) and CEE(cross entrophy error)(교차 엔트로피 오차).\n",
    "\n",
    "1. SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49fe4b-1c71-42ba-8bf1-e776bf4e82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_square_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a1c8e-6432-48b0-abcb-b6cc0727a1f4",
   "metadata": {},
   "source": [
    "2. CEE\n",
    "\n",
    "When the np.log() function is given 0 as input, it returns negative infinity(-inf), making further calculations impossible. To prevent this, a very small value, often denoted as delta, is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8971bd-9b07-4d4d-a0e5-249b8c06df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c64a22-7811-455a-8c4e-64fc715db1ae",
   "metadata": {},
   "source": [
    "3. Mini-batch\n",
    "\n",
    "Calculating the total loss function for a dataset containing a large amount of training data is inefficient.\n",
    "Therefore, using randomly selected subsets of data, called \"mini-batches\", to approximate the total loss functions and using them for training is called \"mini-batch training\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da59ba9e-3d0d-4ee3-859d-253d474bb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10, 784)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) \n",
    "#To use one-hot encoding, one_hot_label=True\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(t_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf5fcc-0522-4e8a-aa92-d4091951c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09031666666666667, 0.0892\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)  \n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" +str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b118b-99df-444d-9e58-cdb9fb7e3705",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608067e-1147-45f1-aa92-e77e0697db3a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
